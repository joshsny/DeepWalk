\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\providecommand {\FN@pp@footnotehinttrue }{}
\providecommand {\FN@pp@footnote@aux }[2]{}
\HyPL@Entry{1<</S/D>>}
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\citation{deepwalk}
\citation{zachary1977}
\citation{maaten2008}
\citation{deepwalk}
\citation{zachary1977}
\citation{maaten2008}
\citation{deepwalk}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Two-dimensional network embeddings calculated using DeepWalk (2014, \cite  {deepwalk}) on Zachary's Karate network (1977, \cite  {zachary1977}). There is a strong correspondence between community structure in the original graph and in the embedding space. Vertex colours represent a modularity-based clustering of the input graph. In practice, network embeddings have dimensions in the tens to low-hundreds and t-SNE (2008, \cite  {maaten2008}) is used here to visualise these embeddings in two-dimensional space.}}{2}{figure.1}\protected@file@percent }
\citation{bai2020}
\citation{zhao2016}
\citation{xu2019}
\citation{qiu2018}
\citation{sajjad2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Essay Outline}{3}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Preliminaries}{3}{subsection.1.2}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\numberline {\let \autodot \@empty }Definition\thmtformatoptarg {(Partially Labelled) Network Graph}}{4}{thmt@dummyctr.dummy.1}\protected@file@percent }
\pgfsyspdfmark {pgfid1}{8209694}{38349497}
\FN@pp@footnotehinttrue 
\citation{deepwalk}
\citation{mikolov2013efficient}
\citation{mikolov2013distributed}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\citation{mikolov2013distributed}
\@writefile{toc}{\contentsline {section}{\numberline {2}Understanding DeepWalk}{5}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Introduction to SkipGram}{5}{subsection.2.1}\protected@file@percent }
\FN@pp@footnote@aux{1}{5}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The structure of the neural network that is trained to obtain the weight matrix $W$ used for the network embeddings in the SkipGram algorithm. The weight matrix $W \in {\mathbb  {R}}^{|V| \times d}$ is represented by the orange lines between the input layer and the hidden layer which has $d$ neurons.}}{6}{figure.2}\protected@file@percent }
\citation{morin2005}
\citation{huffman1952}
\citation{mikolov2013efficient}
\citation{mikolov2013distributed}
\citation{mikolov2013distributed}
\citation{mikolov2013distributed}
\citation{levy&goldberg}
\FN@pp@footnote@aux{2}{7}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces DeepWalk}}{7}{algorithm.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}SkipGram as Matrix Factorisation}{8}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Objective of SGNS}{8}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{loe}{\contentsline {notation}{\numberline {\let \autodot \@empty }Notation}{8}{thmt@dummyctr.dummy.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Finding the Similarity Function Learned by SkipGram}{8}{subsubsection.2.2.2}\prot