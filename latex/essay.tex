\documentclass[a4paper]{article}

\def\npart{III Essay}

\def\ntitle{Walking Deeper on Dynamic Graphs}

\def\ndate{\today}

\input{header}

\let\SO\undefined
\usepackage{tkz-graph}

\newcommand{\shadow}{\partial}
\renewcommand{\P}{\mathbb P}

\begin{document}

\input{titlepage}

\tableofcontents

\section{Motivation}
The motivation for studying nodes in graphs and their representations comes from the desire to understand networks of people or objects and their relations. The motivating question for this essay is
\begin{question}[Motivating Question]
  Given a large network of people how can we quantify the relationships between them?
\end{question}
The natural way to go about this is to let nodes represent persons and let edges
between them represent connections from which we can induce some understanding
of relationship or trust between two people. This task is difficult; even with a
relatively small number of people it is not a task on which humans perform very
well and the task rapidly becomes difficult as the number of nodes in the graph increases.\\
Many of the networks which we wish to study are dynamic; that is they change with time. In a large network it is common that small changes occur during each epoch of time that over time cause larger changes to the network structure. How do we quantify these changes and their impact on the relationships in the network without having to completely re-analyse the network after ever epoch of time, losing the information that we already learned. A frequent example upon which we can draw similarity is that of social networks. A large social network has new users (nodes) being added and new relationships (edges) formed in each epoch of time, however in any given short time period the graph representing the social network does not change substantially. It would be both foolish and costly to re-analyse the graph after each epoch however most of the previous literature has focused on static graphs. In the latter half of this essay I will give an account of recent progress into the application of DeepWalk and similar algorithms to dynamic graphs.
\section{Summary}
There are three main objectives of this essay:
\begin{itemize}
\item Firstly the essay will give a mathematical outline of the DeepWalk
  algorithm and it's application to social representation learning. We will
  briefly discuss the benefits and drawbacks of the algorithm.
\item Secondly we will look at an implementation of DeepWalk to dynamic
  graphs. The challenge here is to develop an unbiased representation of a
  graph at time $t+1$ given it's representation at time $t$, without
  re-analysing the entire network. This is a very important task since many
  networks, especially social ones, are constantly changing. However in any given
  epoch of time the network structure is unlikely to undergoe dramatic change and
  so a computationally effective algorithm will not re-analyse the network at
  each step.
\item Thirdly, we will exhibit an implementation of the outlined dynamic DeepWalk
  application to a social data set [which data set?] and give suggestions as
  to good applications of Dynamic DeepWalking. [talk here about the
  application once I have found one]
\end{itemize}

\section{DeepWalk}
This section gives an outline of the social representation learning algorithm
DeepWalk, first introduced in the seminal paper DeepWalk: Online Learning of Social
Representations by B. Perozzi et. al. \cite{deepwalk}. The method proposed in
this paper not only demonstrated performance improvements from previous methodologies but
also motivated an entirely different approach. At the time of the paper being
written, significant advancements were being made in natural languade processing (NLP)
and the idea of word embeddings was becoming popular through an embedding
algorithm known as word2vec \cite{mikolov2013efficient,mikolov2013distributed}.
DeepWalk implements this algoritm but replaces the idea of the context of a word in
a sentence with the context of a node in a random walk on a graph. This is the
crucial concept of DeepWalk from which the remaining details naturally follow.

The original paper on DeepWalk is lacking in a mathematical underpinning and in
this section we will model the algorithm mathematically. It is suggested that the reader
is familiar with the concepts outlined in the paper by Perozzi et. al. prior to
reading this (more) mathematical exposition. I have endevoured to use similar
notation to the original paper to ease cross-referencing. Without further ado, let us begin
our journey.

\begin{definition}
  Let $G = (V, E)$ be an undirected graph (representing a network). $V$ represents the
  members of the network, commonly referred to as the nodes and $E \subset V
  \times V$ represents their connections.\\

  The nodes and edges have lables and $G_L = (V, E, X, Y)$ represents the
  partially labelled network. $X \in \R^{|V| \times S}$ where $S$ is the size of
  the feature space for each attribute vector and $Y \in \R^{|V| \times
    |\mathcal{Y}|}$, where $\mathcal{Y}$ is the set of labels.
\end{definition}

Our goal is to learn $X_E \in \R mb^{|V| \times d}$ where $d$ is a small number
of latent dimensions. The idea is that each latent dimension contributes a 
dimensional

\subsection{SkipGram}
To understand DeepWalk mathematically, we first need to understand what the SkipGram
model is doing, since this is the model underpinning DeepWalk, which took it
from NLP and applied it to graphs. It is easiest to first gain an understanding
of SkipGram in it's original context.\\

Paragraph with word analogies:
Skipgram trains a neural network to do the following task. Given an input word,
located somewhere in a sentence, pick a nearby word at random. The task of the
network is to give the probability of each word in the vocabulary being the
nearby word that we choose. As an example, if the context work is ``rainy'' then
``weather'' would be assigned a high probability whereas ``carrot'' would be
assigned a low one.

Paragraph without word analogies:
In the context of graph networks, SkipGram trains a neural network to do the
following task.\\
Given an input vertex $v$ and a random walk, $W_{v}$, of length $2t+1$ with $v$
at it's centre. Pick a nearby vertex at random. The task of the neural network is to
predict the probability that each vertex in $V$ will be the randomly chosen
vertex. Therefore, verticies far away on the graph that correspond to unfamiliar
nodes are unlikely to co-occur on the same random walk and will be assigned a
low probability. Conversely, nearby and well connected vertices are likely to
co-occur on a random walk with input $v$ and thus will be assigned higher
probabilities. This allows us to train a network with weights that represent the
connectedness between nodes on the graph.

The neural network is trained by feeding it pairs of nodes $(v, w)$ where $v$
represents the input node and $w$ is a context node, which lies within a certain
window size.

To formalise this, each of the nodes $v \in V$ are represented by a one-hot
encoding vector $e_v \in \mathbb{R}^{|V|}$ allowing us to feed $e_v$ into the
neural network. When $e_v$ is fed into the network, a single linear hidden layer with
$d$ neurons is used, where $d$ is the desired dimension of the latent
representations, which is then passed to a softmax classifier for output. The
output of the network is a vector $o \in \mathbb{R}^d$ containing the estimated
probabilities that a randomly selected nearby word is that vocabulary word.

The idea behind having a linear hidden layer, which does not use an activation
function, is to use the resulting weight matrix $W \in \mathbb{R}^{|V| \times
  d}$ as the embedding vectors for the nodes in the graph. This is intuitive as
the hidden layer acts as a bottleneck that tries to represent as much
information as possible to distinguish the nodes, but is only allowed $d$
neurons to do so. Since $d \ll |V|$ there is a low risk of overfitting.

*** Is our vocabulary the whole graph, or is it just the random walk that we
feed to the SkipGram model, perhaps it is just the random walk, be careful with
this! ***

*** Create here an image resembling the one found in the blog post on SkipGram
but for graphs and with $d$ dimensions ***

The algorithm used in DeepWalk varies slightly from the SkipGram algorithm
discussed. Firstly, calculating the normalization factor
in the Softmax layer requires a computational complexity of $O(|V|)$, this is
reduced by using Hierarchical Softmax to approximate the Softmax probabilities,
requiring a complexity of only $O(log|V|)$. In particular, a Huffman coding is
used to reduce the access time of frequent elements in the tree, as suggested by
Mikolov et al. in the original Word2Vec
papers.\cite{mikolov2013distributed,mikolov2013efficient}

Here I exhibit a proof that DeepWalk with Softmax is equivalent to factorising a
certain matrix $M$. This is a result that, though mentioned, I have not found
explicitely proved in the literature.

\begin{theorem}
  SkipGram with Softmax is implicitely factorising the matrix
  \[M = \]
\end{theorem}

In the above theorem, $\mathcal{D}$ represents the random walk corpus and
$\#(v,c)$, $\#(v)$ and $\#(c)$ denote the number of times vertex-context pair
$(v,c)$, vertex $v$ and context $c$ appear in the corpus respectively.

\begin{proof}

  *** Make a note on the assumptions of the proof, should be done in the proof
  itself ***

\end{proof}


In later adaptations of DeepWalk, Negative Sampling is used instead of
Hierarchical Softmax. This was shown by Levy and
Goldberg\cite{levy&goldberg}, in a similar manner to the above proof, to be
implicitly factorising a different matrix $M$ to the one above.
*** Do I need to explain what Negative Sampling is? I don't have much space, but
a sentence or two would likely help the reader. ***

*** At some point I need to actually outline these algorithms as the other
papers have done, there needs to be an explicit DeepWalk algorithm ***

\begin{theorem}[Levy, Goldberg (2014)]
  SkipGram with Negative Sampling (SGNS) is implicitly factorising the matrix
  \[M = \log{\frac{\#(v,c)|\mathcal{D}|}{\#(v)\#(c)}} - \log{b} = WC^T\in \mathbb{R}^{|V|
      \times |V|}\]
  where $W \in \mathbb{R}^{|V| \times d}$ and $|C| \in \mathbb{R}^{d \times |V|}$
  and $b$ is the number of negative samples.
\end{theorem}

*** Make a note on the assumptions of the proof ***

Most interestingly, the resulting expression for $M$ is that of a pointwise
mutual information (PMI) matrix shifted by a factor of $\log k$. PMI was
introduced as a measure of association between words in 1990 by Church and Hanks
\cite{church1990} and became widely adopted for NLP tasks.\\

In the remainder of this essay, when referring to DeepWalk, it will be
implicitly assumed that Negative Sampling is used as appose to Hierarchical
Softmax. This is  because Negative Sampling has been shown to be more efficient
and therefore much of the literature has adopted DeepWalk with SGNS. This convention will also
serve us well when we look to applying DeepWalk to dynamic graphs since here
Negative Sampling is also applied.

\subsection{DeepWalk as matrix factorisation}

We continue to look at DeepWalk in the context of matrix factorisation. Much of the proceeding analysis was exhibited in a recent paper by Qiu et
al.\cite{qiu2018} published in 2018 building upon work by Yang et al.\cite{yang2015}
from 2015. The aim of the former paper was to lay the foundationds for, and
unify, the SkipGram based network embedding methods.\\

In their paper, Qiu et al. gave a theoretical understanding of the DeepWalk
algorithm by proving the following theorem:

\begin{theorem}
  As $L \to \infty$, DeepWalk is equivalent to factorising
  \[\log{\left(\frac{vol(G)}{T}\left( \sum_{r = 1}^T P^r  \right) D^{-1}
    \right)} - \log{b}\]
  where $D = diag(d_1, \dots, \d_{|V|})$ and $P = D^{-1}A$.
\end{theorem}

*** Give the conditions for the theorem to hold. ***

What follows is a careful outline of this proof.

\section{Matters of Convergence}
\section{DeepWalk Sucks}
\section{Dynamic DeepWalking}
In this section we transition away from the static implementation of DeepWalk
towards network embeddings for Dynamic datasets. In particular, the focus of the
section is to introduce an adaptation of DeepWalk to dynamic datasets as
proposed in the paper published by Sajjad et al.\cite{sajjad2019} in early 2019.


\section{Discussion of an application}

The aim of this section is to bring the theory discussed on Dynamic DeepWalking
into practice.

\bibliography{references}
\bibliographystyle{ieeetr}



\printindex
\end{document}