\documentclass[a4paper]{article}

\def\npart{III Essay}

\def\ntitle{Walking Deeper on Dynamic Graphs}

\def\ndate{\today}

\input{header}

\let\SO\undefined
\usepackage{tkz-graph}

\newcommand{\shadow}{\partial}
\renewcommand{\P}{\mathbb P}
\renewcommand{\E}{\mathbb E}
\renewcommand{\D}{\mathcal{D}}

\begin{document}

\input{titlepage}

\tableofcontents

\section{Motivation}
The motivation for studying nodes in graphs and their representations comes from the desire to understand networks of people or objects and their relations. The motivating question for this essay is
\begin{question}[Motivating Question]
  Given a large network of people how can we quantify the relationships between them?
\end{question}
The natural way to go about this is to let nodes represent persons and let edges
between them represent connections from which we can induce some understanding
of relationship or trust between two people. This task is difficult; even with a
relatively small number of people it is not a task on which humans perform very
well and the task rapidly becomes difficult as the number of nodes in the graph increases.\\
Many of the networks which we wish to study are dynamic; that is they change with time. In a large network it is common that small changes occur during each epoch of time that over time cause larger changes to the network structure. How do we quantify these changes and their impact on the relationships in the network without having to completely re-analyse the network after ever epoch of time, losing the information that we already learned. A frequent example upon which we can draw similarity is that of social networks. A large social network has new users (nodes) being added and new relationships (edges) formed in each epoch of time, however in any given short time period the graph representing the social network does not change substantially. It would be both foolish and costly to re-analyse the graph after each epoch however most of the previous literature has focused on static graphs. In the latter half of this essay I will give an account of recent progress into the application of DeepWalk and similar algorithms to dynamic graphs.
\section{Summary}
There are three main objectives of this essay:
\begin{itemize}
\item Firstly the essay will give a mathematical outline of the DeepWalk
  algorithm and it's application to social representation learning. We will
  briefly discuss the benefits and drawbacks of the algorithm.
\item Secondly we will look at an implementation of DeepWalk to dynamic
  graphs. The challenge here is to develop an unbiased representation of a
  graph at time $t+1$ given it's representation at time $t$, without
  re-analysing the entire network. This is a very important task since many
  networks, especially social ones, are constantly changing. However in any given
  epoch of time the network structure is unlikely to undergoe dramatic change and
  so a computationally effective algorithm will not re-analyse the network at
  each step.
\item Thirdly, we will exhibit an implementation of the outlined dynamic DeepWalk
  application to a social data set [which data set?] and give suggestions as
  to good applications of Dynamic DeepWalking. [talk here about the
  application once I have found one]
\end{itemize}

\section{DeepWalk}
This section gives an outline of the social representation learning algorithm
DeepWalk, first introduced in the seminal paper DeepWalk: Online Learning of Social
Representations by B. Perozzi et. al. \cite{deepwalk}. The method proposed in
this paper not only demonstrated performance improvements from previous methodologies but
also motivated an entirely different approach. At the time of the paper being
written, significant advancements were being made in natural languade processing (NLP)
and the idea of word embeddings was becoming popular through an embedding
algorithm known as word2vec \cite{mikolov2013efficient,mikolov2013distributed}.
DeepWalk implements this algoritm but replaces the idea of the context of a word in
a sentence with the context of a node in a random walk on a graph. This is the
crucial concept of DeepWalk from which the remaining details naturally follow.

The original paper on DeepWalk is lacking in a mathematical underpinning and in
this section we will model the algorithm mathematically. It is suggested that the reader
is familiar with the concepts outlined in the paper by Perozzi et. al. prior to
reading this (more) mathematical exposition. I have endevoured to use similar
notation to the original paper to ease cross-referencing. Without further ado, let us begin
our journey.

\begin{definition}
  Let $G = (V, E)$ be an undirected graph (representing a network). $V$ represents the
  members of the network, commonly referred to as the nodes and $E \subset V
  \times V$ represents their connections.\\

  The nodes and edges have lables and $G_L = (V, E, X, Y)$ represents the
  partially labelled network. $X \in \R^{|V| \times S}$ where $S$ is the size of
  the feature space for each attribute vector and $Y \in \R^{|V| \times
    |\mathcal{Y}|}$, where $\mathcal{Y}$ is the set of labels.
\end{definition}

Our goal is to learn $X_E \in \R mb^{|V| \times d}$ where $d$ is a small number
of latent dimensions. The idea is that each latent dimension contributes a 
dimensional

\subsection{Introduction to SkipGram}
To understand DeepWalk mathematically, we first need to understand what the SkipGram
model is doing, since this is the model underpinning DeepWalk, which took it
from NLP and applied it to graphs. In the context of graph networks, SkipGram trains a neural network to do the
following task:\\
Given an input vertex $v$ and a random walk, $W_{v}$, of length $2t+1$ with $v$
at it's centre. Pick a nearby vertex at random. The task of the neural network is to
predict the probability that each vertex in $V$ will be the randomly chosen
vertex. Therefore, verticies far away on the graph that correspond to unfamiliar
nodes are unlikely to co-occur on the same random walk and will be assigned a
low probability. Conversely, nearby and well connected vertices are likely to
co-occur on a random walk with input $v$ and thus will be assigned higher
probabilities. This allows us to train a network with weights that represent the
connectedness between nodes on the graph. The neural network is trained by
feeding it pairs of nodes $(v, c)$ where $v$ represents the input node and $c$ is a context node, which lies within distance
$t$ of the vertex $v$.\\ 

To formalise this, each of the nodes $v \in V$ are represented by a one-hot
encoding vector $e_v \in \mathbb{R}^{|V|}$ allowing us to feed $e_v$ into the
neural network. When $e_v$ is fed into the network, a single linear hidden layer with
$d$ neurons is used, where $d$ is the desired dimension of the latent
representations, which is then passed to a softmax classifier for output. The
output of the network is a vector $o \in \mathbb{R}^d$ containing the estimated
probabilities that a randomly selected nearby word is that vocabulary word.

The idea behind having a linear hidden layer, which does not use an activation
function, is to use the resulting weight matrix $W \in \mathbb{R}^{|V| \times
  d}$ as the embedding vectors for the nodes in the graph. This is intuitive as
the hidden layer acts as a bottleneck that tries to represent as much
information as possible to distinguish the nodes, but is only allowed $d$
neurons to do so. Since $d \ll |V|$ there is a low risk of overfitting.

*** Is our vocabulary the whole graph, or is it just the random walk that we
feed to the SkipGram model, perhaps it is just the random walk, be careful with
this! ***

*** Create here an image resembling the one found in the blog post on SkipGram
but for graphs and with $d$ dimensions ***

The algorithm used in DeepWalk varies slightly from the SkipGram algorithm
discussed. Calculating the normalization factor in the Softmax layer requires a
computational complexity of $O(|V|)$, in the original DeepWalk paper this is
reduced by using Hierarchical Softmax to approximate the softmax probabilities,
requiring a complexity of only $O(log|V|)$. In particular, a Huffman coding is
used to reduce the access time of frequent elements in the tree, as suggested by
Mikolov et al. in the original Word2Vec
papers.\cite{mikolov2013distributed,mikolov2013efficient}

In later adaptations of DeepWalk, SkipGram with Negative Sampling (SGNS) is used instead of
Hierarchical Softmax. In the remainder of this essay, when referring to DeepWalk, it will be
implicitly assumed that Negative Sampling is used as appose to Hierarchical
Softmax. This is because SGNS has been shown to be more efficient
and therefore has been adopted by much of the further literature. This convention will also
serve us well when we look at applying DeepWalk to dynamic graphs since here
Negative Sampling is also applied.\\

\subsection{SkipGram as matrix factorisation}
*** Do I need to explain what Negative Sampling is? I don't have much space, but
a sentence or two would likely help the reader. ***\\

In this section we will exhibit a proof that SGNS is
equivalent to factorising a certain matrix $M$ into two smaller matricies $W$
and $C$ where the rows in $W$ correspond to the learned embedding of each vertex. This result was first proved by Levy and
Goldberg\cite{levy&goldberg} in the context of word embeddings.\\
\subsubsection{Objective of SGNS}
Given an arbitrary input-context pair $(v,c)$ the objective is to determine if
the pair comes from the random-walk corpus $\mathcal{D}$.\\
Let $P(\D = 1 | v, c)$ denote the probability that $(v,c)$ comes from a random
walk on the graph and $P(\D = 0| v, c)$
the probability it does not. Then the distribution is modelled by a signoid
function
\[P(D = 1 | w, c) = \sigma(\vec{v} \cdot \vec{c}) = \frac{1}{1 + e^{-\vec{v} \cdot \vec{c}}}\]
where $\vec{v}$ and $\vec{c}$ are $d$-dimensional vectors to be learned. SGNS attempts to maximise $P(\mathcal{D} = 1 | w,c)$ for observed pairs $(v, c)$
whilst simultaneously maximising $P(\D = 0 | v, c)$ for randomly sampled
negative examples.\\
It assumes that randomly selecting a context $c$ for a given
node $v$ is likely to result in an unobserved pair $(v,c)$. In the context of
social networks, this assumption is reasonable since the networks are almost
always sparse (The number of edges is usually $O(|V|)$). However in a different
context, if the network is dense, then this may be an unreasonable assumption.\\

According to this assumption, the objective function of SGNS for a single
observation $(v,c)$ is:

\[\log{\sigma(\vec{w} \cdot \vec{c})} + b \cdot \E_{c_{N} \sim P_D}\log{\sigma(-\vec{v} \cdot \vec{c})}\]
where the minus sign comes from the fact that $1 - \sigma(x) = \sigma(-x)$, $b$
is the number of negative samples and $c_N$ is the sampled context node, drawn
according to $P_D (c) = \frac{\#(c)}{| \D |}$ which is known as the unigram
distribution.\\

\begin{notation} $\#(v,c)$, $\#(v)$ and $\#(c)$ denote the number of times vertex-context pair
  $(v,c)$, vertex $v$ and context $c$ appear in the generated random-walk corpus
  $\mathcal{D}$ respectively.
\end{notation}

This objective function is trained using stochastic gradient descent with
updates after each  observed pair in the random-walk corpus $\D$. The resulting
global objective becomes
\begin{equation}
  l = \sum_{v \in V} \sum_{c \in V}\#(w,c)\log{\sigma(\vec{w} \cdot \vec{c})} +
  b \cdot \E_{c_{N} \sim P_D}\log{\sigma(-\vec{v} \cdot \vec{c})}
\end{equation}

\subsubsection{Finding the appropriate relational matrix}

If we let $W$ be the matrix with rows $w_i$ and $C$ the matrix with columns
$c_i$ then SGNS can be interpreted as factorising a matrix $M = WC^T$. An entry in the matrix $M_{ij}$ corresponds to the dot product $\vec{w_i} \cdot
\vec{c_j}$. Therefore SGNS is factorising a matrix in which each row corresponds
to an input node $v_i \in |V|$ and each column to a context node $v_j \in |V|$ and the value of $M_{ij}$ expresses the
strength of association between the input-context pair $(v_i, v_j)$ using some similarity
function $s(v_i,v_j)$. 
\begin{theorem}[Levy, Goldberg (2014)]
  SkipGram with Negative Sampling (SGNS) is implicitly factorising a matrix $M =
  WC^T$ with
  \[M_{ij} = \log{\frac{\#(v,c)|\mathcal{D}|}{\#(v)\#(c)}} - \log{b} = WC^T\in \mathbb{R}^{|V|
      \times |V|}\]
  where $W \in \mathbb{R}^{|V| \times d}$,$|C| \in \mathbb{R}^{d \times |V|}$ and $b$ is the number of negative samples.
\end{theorem}

\begin{proof}

  *** Make a note on the assumptions of the proof

\end{proof}

Most interestingly, the resulting expression for $M$ is that of a pointwise
mutual information (PMI) matrix shifted by a factor of $\log k$. PMI was
introduced as a measure of association between words in 1990 by Church and Hanks
\cite{church1990} and became widely adopted for NLP tasks.\\

There is an equivalent theorem for SkipGram with Softmax that was proved by Yang
et al.\cite{yangalternative2015} and was later used in their development of
text-associated DeepWalk (TADW)\cite{yang2015} which encorporates text features
of the verticies in a social graph.

\begin{theorem}[Yang et al. (2015)]
  SkipGram with Softmax is implicitely factorising the matrix
  \[M = \log{\frac{\#(v,c)}{\#(v)}}\]
\end{theorem}

The proof of this follows very similarly to the previous theorem and will not be
shown here since we will only be concerned with SGNS.



*** At some point I need to actually outline these algorithms as the other
papers have done, there needs to be an explicit DeepWalk algorithm ***

\subsection{DeepWalk as matrix factorisation}

We continue to look at DeepWalk in the context of matrix factorisation. Much of the proceeding analysis was exhibited in a recent paper by Qiu et
al.\cite{qiu2018} published in 2018 building upon work by Yang et al.\cite{yang2015}
from 2015. The aim of the former paper was to lay the foundationds for, and
unify, the SkipGram based network embedding methods.\\

In their paper, Qiu et al. gave a theoretical understanding of the DeepWalk
algorithm by proving the following theorem:

\begin{theorem}
  As $L \to \infty$, DeepWalk is equivalent to factorising
  \[\log{\left(\frac{vol(G)}{T}\left( \sum_{r = 1}^T P^r  \right) D^{-1}
      \right)} - \log{b}\]
  where $D = diag(d_1, \dots, \d_{|V|})$ and $P = D^{-1}A$.
\end{theorem}

*** Give the conditions for the theorem to hold. ***

What follows is a careful outline of this proof.

\section{Matters of Convergence}
\section{DeepWalk Sucks}
\section{Dynamic DeepWalking}
In this section we transition away from the static implementation of DeepWalk
towards network embeddings for Dynamic datasets. In particular, the focus of the
section is to introduce an adaptation of DeepWalk to dynamic datasets as
proposed in the paper published by Sajjad et al.\cite{sajjad2019} in early 2019.


\section{Discussion of an application}

The aim of this section is to bring the theory discussed on Dynamic DeepWalking
into practice.

\bibliography{references}
\bibliographystyle{ieeetr}



\printindex
\end{document}