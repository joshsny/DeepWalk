{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import Graph\n",
    "import Classifier\n",
    "import argparse\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter\n",
    "from datetime import timedelta\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(G, max_paths, path_len, save_walks):\n",
    "    #Build corpus\n",
    "    print(\"\\t**Stage 1 : Generating random walks**\")\n",
    "    t1 = perf_counter()\n",
    "    corpus = Graph.build_walk_corpus(G=G, max_paths=max_paths, path_len=path_len)\n",
    "    t2 = perf_counter()\n",
    "    print(\"\\nNumber of walks in the corpus = \",len(corpus))\n",
    "    print(\"Time Elapsed for building walk corpus --> \", timedelta(seconds=t2-t1))\n",
    "    if save_walks:\n",
    "        Graph.save_corpus(max_paths, path_len, corpus)\n",
    "    print(\"---------------------------------------\\n\")\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(fname):\n",
    "    try:\n",
    "        word_vec = KeyedVectors.load_word2vec_format(fname, binary=False)\n",
    "        print(\"Embeddings successfully loaded from \"+fname)\n",
    "        return word_vec, True\n",
    "    except IOError:\n",
    "        print(\"Embedding file not found. Proceeding to generate new embeddings\")\n",
    "        # Y/N here\n",
    "        return _, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_embeddings(d,w,hs,corpus,save_emb):\n",
    "    #Train model\n",
    "    #ToDO: try negative sampling (hs=0)\n",
    "    print(\"\\t**Stage 2 : Generating Embeddings for nodes using Word2Vec**\")\n",
    "    print(\"\\nWord2Vec parameters : Dimensions = \"+str(d)+\", window = \"+str(w)+\", hs = \"+str(hs)+\", number of cpu cores assigned for training = \"+str(cpu_count()))\n",
    "    \n",
    "    t1 = perf_counter()\n",
    "    model = Word2Vec(size = d, window=w, sg=1, min_count=0, hs=hs, compute_loss=True, workers=cpu_count())\n",
    "    model.build_vocab(corpus)\n",
    "    model.train(corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    t2 = perf_counter()\n",
    "    \n",
    "    print(\"Model training done. Word2Vec embeddings generated.\") \n",
    "    print(\"Time Elapsed for generating embeddings --> \", timedelta(seconds=t2-t1))\n",
    "    \n",
    "    word_vec = model.wv\n",
    "    \n",
    "    if save_emb:\n",
    "        #Save w2v embeddings\n",
    "        name = 'word2vec-d'+str(d)+'-w'+str(w)+'-hs'+str(hs)+'.txt'\n",
    "        word_vec.save_word2vec_format(binary=False,fname=name)\n",
    "        print(\"Embeddings saved to file -> \",name)\n",
    "\n",
    "    print(\"---------------------------------------\\n\")\n",
    "    \n",
    "    return word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classifier(G, subs_coo, word_vec):\n",
    "    #Sometimes the model doesn't predict anything at all for some inputs. Its either the model's fault or that user has no subscriptions at\n",
    "    #all, in that case the model is predicting properly but of course a zero output would raise exceptions during sklearn's\n",
    "    #F1 score function.\n",
    "    #Currently evaluating performance with OVR Logistic Regression.\n",
    "    print(\"\\t**Stage 3 : Evaluating classifier performance with the embeddings**\")\n",
    "\n",
    "    t1 = perf_counter()\n",
    "    results = Classifier.evaluate(G, subs_coo, word_vec)\n",
    "    t2 = perf_counter()\n",
    "\n",
    "    print(\"\\n Evaluation completed using the following:\")\n",
    "    for i in results.keys():\n",
    "        print(\"--> \",i)\n",
    "    print(\"Time Elapsed for evaluation --> \", timedelta(seconds=t2-t1))\n",
    "    print(\"---------------------------------------\\n\")\n",
    "\n",
    "    print(\"Printing evaluation results : \")\n",
    "    trainsize = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    print(results.items())\n",
    "    for (name,res) in results.items():\n",
    "        print(\"\\n\\nClassifier : \",name)\n",
    "        for (tr_size,res_) in zip(trainsize,res):\n",
    "            print(\"\\tTraining size : \",tr_size)\n",
    "            print(\"\\t\\tMicro F1: \",res_[0])\n",
    "            print(\"\\t\\tMacro F1: \",res_[1])\n",
    "        \n",
    "        avg = np.average(res,axis=0)\n",
    "        print(\"\\t---------------------------------------\")\n",
    "        print(\"\\t Average Micro F1 : \",avg[0])\n",
    "        print(\"\\t Average Macro F1 : \",avg[1])\n",
    "        # Classifier.plot_graph(trainsize, res)\n",
    "        print(\"====================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(args):\n",
    "\n",
    "    dimensions = args.d\n",
    "    max_paths = args.walks\n",
    "    path_len = args.len\n",
    "    window = args.window\n",
    "    hs = args.hs\n",
    "    save_walks = args.w\n",
    "    save_emb = args.e\n",
    "    load_corpus = args.lw\n",
    "    load_emb = args.le\n",
    "    readCorpusFlag = False\n",
    "    readEmbedFlag = False\n",
    "    \n",
    "    #Open and parse dataset\n",
    "    G, subs_coo = Graph.parse_mat_file('../data/blogcatalog.mat')\n",
    "\n",
    "    if load_emb:\n",
    "        word_vec, readEmbedFlag = load_embeddings(load_emb)\n",
    "\n",
    "        #if there's error in reading the specified embedding file, build corpus and generate embeddings\n",
    "        #unless interrupted by user\n",
    "        if readEmbedFlag == False:\n",
    "            #Build Corpus \n",
    "            corpus = build_corpus(G, max_paths=args.walks, path_len=args.len, save_walks=args.w)\n",
    "            #Train word2vec model and generate embeddings\n",
    "            word_vec = generate_embeddings(dimensions, window, hs, corpus, save_emb)\n",
    "\n",
    "    #If no embedding file is given\n",
    "    else:    \n",
    "        #If corpus file is specified\n",
    "        if load_corpus:\n",
    "                corpus, max_paths, path_len, readCorpusFlag = Graph.load_corpus(G,load_corpus)\n",
    "\n",
    "        #If corpus file is not specified or if loading the file fails\n",
    "        if readCorpusFlag == False:\n",
    "                corpus = build_corpus(G, max_paths=args.walks, path_len=args.len, save_walks=args.w)\n",
    "        \n",
    "        #Generate new Embeddings\n",
    "        word_vec = generate_embeddings(dimensions, window, hs, corpus, save_emb)\n",
    "\n",
    "    #Evaluate the embeddings by passing it through classifier(s)\n",
    "    eval_classifier(G, subs_coo, word_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    parser = argparse.ArgumentParser(\"DeepWalk\", description = \"Implementation of \"+ \n",
    "        \"DeepWalk model. File Author: Apoorva\")\n",
    "    parser.add_argument(\"--d\", default=128, type=int, help=\"Dimensions of word embeddings\")\n",
    "    parser.add_argument(\"--walks\", default=10, type=int, help=\"Number of walks per node\")\n",
    "    parser.add_argument(\"--len\", default=30, type=int, help=\"Length of random walk\")\n",
    "    parser.add_argument(\"--window\", default=5, type=int, help=\"Window size for skipgram\")\n",
    "    parser.add_argument(\"--hs\", default=1, type=int, help=\"0 - Negative Sampling  1 - Hierarchical Softmax\")\n",
    "    parser.add_argument(\"--lw\", default='', help=\"Load random walk corpus from file\")\n",
    "    parser.add_argument(\"--le\", default='', help=\"Load embeddings from file\")\n",
    "    parser.add_argument(\"-w\", action='store_true', help=\"Flag to save random walk corpus to disk\")\n",
    "    parser.add_argument(\"-e\", action='store_true', help='Flag to save word embeddings to disk')\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    #Enter cmd line arguments here. Leave it blank if you want default parameters.\n",
    "    #Eg : \"--d 64 -w -e\"\n",
    "    cmdargs = \"--d 128 -w -e\"\n",
    "    args = parser.parse_args(cmdargs.split())\n",
    "    \n",
    "    process(args)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ../data/blogcatalog.mat\n",
      "Type: Graph\n",
      "Number of nodes: 10312\n",
      "Number of edges: 333983\n",
      "Average degree:  64.7756\n",
      "---------------------------------------\n",
      "\n",
      "\t**Stage 1 : Generating random walks**\n",
      "Building walk corpus with parameters : max_paths per node =  10  and path_length =  30\n",
      "Completed\n",
      "\n",
      "Number of walks in the corpus =  103120\n",
      "Time Elapsed for building walk corpus -->  0:00:22.991148\n",
      "Corpus saved on disk as ../random_walks/RandomWalks-w10-l30.txt\n",
      "---------------------------------------\n",
      "\n",
      "\t**Stage 2 : Generating Embeddings for nodes using Word2Vec**\n",
      "\n",
      "Word2Vec parameters : Dimensions = 128, window = 5, hs = 1, number of cpu cores assigned for training = 4\n",
      "Model training done. Word2Vec embeddings generated.\n",
      "Time Elapsed for generating embeddings -->  0:01:25.770663\n",
      "Embeddings saved to file ->  word2vec-d128-w5-hs1.txt\n",
      "---------------------------------------\n",
      "\n",
      "\t**Stage 3 : Evaluating classifier performance with the embeddings**\n",
      "\n",
      " Evaluation completed using the following:\n",
      "-->  Logistic_Regression\n",
      "Time Elapsed for evaluation -->  0:00:27.594677\n",
      "---------------------------------------\n",
      "\n",
      "Printing evaluation results : \n",
      "dict_items([('Logistic_Regression', [(0.3205334559668889, 0.18651184041900443), (0.3469846207015725, 0.21019177219094623), (0.3667854315122724, 0.22900428663409633), (0.3725060546649752, 0.23696029942664895), (0.37934857934857935, 0.2507898888821809), (0.37933425797503467, 0.2512228149258562), (0.38859138533178117, 0.2585488941170333), (0.3963649073750437, 0.2647448093791031), (0.39262934089298374, 0.24486139957588696)])])\n",
      "\n",
      "\n",
      "Classifier :  Logistic_Regression\n",
      "\tTraining size :  0.2\n",
      "\t\tMicro F1:  0.3205334559668889\n",
      "\t\tMacro F1:  0.18651184041900443\n",
      "\tTraining size :  0.3\n",
      "\t\tMicro F1:  0.3469846207015725\n",
      "\t\tMacro F1:  0.21019177219094623\n",
      "\tTraining size :  0.4\n",
      "\t\tMicro F1:  0.3667854315122724\n",
      "\t\tMacro F1:  0.22900428663409633\n",
      "\tTraining size :  0.5\n",
      "\t\tMicro F1:  0.3725060546649752\n",
      "\t\tMacro F1:  0.23696029942664895\n",
      "\tTraining size :  0.6\n",
      "\t\tMicro F1:  0.37934857934857935\n",
      "\t\tMacro F1:  0.2507898888821809\n",
      "\tTraining size :  0.7\n",
      "\t\tMicro F1:  0.37933425797503467\n",
      "\t\tMacro F1:  0.2512228149258562\n",
      "\tTraining size :  0.8\n",
      "\t\tMicro F1:  0.38859138533178117\n",
      "\t\tMacro F1:  0.2585488941170333\n",
      "\tTraining size :  0.9\n",
      "\t\tMicro F1:  0.3963649073750437\n",
      "\t\tMacro F1:  0.2647448093791031\n",
      "\t---------------------------------------\n",
      "\t Average Micro F1 :  0.37145311486323684\n",
      "\t Average Macro F1 :  0.2369817783945285\n",
      "====================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
